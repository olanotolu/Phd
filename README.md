# Thermodynamic Language Model (TLM) â€” Babel Engine v0.1

**TLM** is a physics-inspired language model that represents text as an energy landscape and generates structure via thermodynamic sampling, instead of standard next-token prediction.

This repo contains **Babel Engine v0.1** â€” a character-level Thermodynamic Language Model trained on random "Library of Babel"â€“style text to demonstrate how **order can emerge from chaos**.

---

## ğŸŒ¡ï¸ What is a Thermodynamic Language Model?

A **Thermodynamic Language Model (TLM)** is an energy-based model over symbolic sequences:

- Each character position is a **discrete variable**.
- Local **factors** (e.g. bigrams) define an **energy** for each sequence.
- **Gibbs / block sampling** is used to move from random noise to low-energy, structured configurations.

Instead of predicting the next token like a Transformer, a TLM asks:

> **"How stable is this whole sequence?"**

**Low energy = stable / structured**  
**High energy = unstable / unlikely**

---

## ğŸ”¬ What this repo shows (v0.1)

Babel Engine v0.1 demonstrates:

- Training a **discrete energy-based model (EBM)** on random text.
- Using **Gibbs sampling** to generate new sequences.
- Quantitatively comparing:
  - pure random text ("Library of Babel" noise) vs
  - TLM-generated samples

Despite being trained on noise, the TLM learns to:

- form **repeating patterns** (motifs)
- stabilize certain bigrams and clusters
- converge to **attractor states** (low-energy sequences)

In other words, the model starts to create **structure out of chaos**.

---

## ğŸ§± Architecture Overview

v0.1 is a **character-level** Thermodynamic Language Model with:

- **Alphabet**: `aâ€“z`, space, comma, period (29 characters)
- **Nodes**: one discrete variable per sequence position
- **Factors**: bigram interactions between neighboring characters
- **Energy**: sum of local factor energies
- **Sampler**: block Gibbs sampling
- **Training**: pseudo-likelihood over observed sequences

See [`ARCHITECTURE.md`](./ARCHITECTURE.md) for detailed architecture specification.

See [`example.py`](./example.py) for a full run.

---

## ğŸ“‚ Repo Structure

```
tlm-babel/
â”œâ”€â”€ dataset.py              # Generate random "Babel" sequences
â”œâ”€â”€ babel_library.py        # Core TLM implementation (EBM + Gibbs)
â”œâ”€â”€ training.py             # Training loop (pseudo-likelihood, KL-gradient)
â”œâ”€â”€ visualization.py        # Pattern analysis and visualization
â”œâ”€â”€ example.py              # End-to-end experiment script
â”œâ”€â”€ babel_exploration.ipynb # Interactive Jupyter notebook
â”œâ”€â”€ ARCHITECTURE.md         # Detailed architecture specification
â””â”€â”€ requirements.txt        # Dependencies
```

---

## â–¶ï¸ Quickstart

```bash
git clone https://github.com/yourname/tlm-babel.git
cd tlm-babel
pip install -r requirements.txt

python example.py
```

You should see output like:

- training progress over epochs
- final data and model energies
- pattern statistics for:
  - random data
  - TLM samples
- sample sequences generated by the TLM

---

## ğŸ§  Why this matters

This project is a **prototype** for a new family of language models:

- **Thermodynamic Language Models (TLMs)**
  - represent text with energy, not logits
  - generate via sampling, not direct prediction
  - naturally fit future **probabilistic hardware** (e.g. Extropic)

Babel Engine v0.1 is a **minimal proof-of-concept**:

> A small EBM over characters can learn to create stable, repeating patterns even when trained on pure random text.

This is the **first step** toward thermodynamic language models and physics-inspired symbolic intelligence.

---

## ğŸš§ Roadmap

Planned directions:

- **v0.2** â€” Train on real text (Wikipedia / domain-specific corpora)
- **v0.3** â€” Multi-scale factors (bigrams + trigrams + span-level)
- **v0.4** â€” Better training (contrastive divergence, KL gradients)
- **v0.5** â€” Anomaly detection on LLM outputs using TLM energy

---

## ğŸ“œ Citation (draft)

If you'd like to reference this work:

```
Adu, O. (2025). Thermodynamic Language Models: Emergent Structure in 
Discrete Energy-Based Text Systems. Work in progress.
```

---

## ğŸ¤ Contributions

This is early-stage research.

Issues, ideas, and pull requests are welcome â€” especially around:

- new factor types
- better samplers
- visualization tools
- real-data experiments

---

## ğŸ“– Related Work

- **Extropic THRML**: Probabilistic computing framework for energy-based models
- **Energy-Based Models**: LeCun et al., "A Tutorial on Energy-Based Learning"
- **Borges's Library of Babel**: Philosophical exploration of infinite text spaces

---

## License

MIT

---

## ğŸ”— Links

- **Research Note**: [Coming soon]
- **Interactive Demo**: [Coming soon]
- **Paper**: [Work in progress]

---

**Tagline**: *"From chaos to structure."*
