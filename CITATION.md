# Citation Information

## Recommended Citation Format

### For Research Papers

```bibtex
@misc{adu2025tlm,
  title={Thermodynamic Language Models: Emergent Structure in Discrete Energy-Based Text Systems},
  author={Adu, O.},
  year={2025},
  note={Work in progress},
  url={https://github.com/yourname/tlm-babel}
}
```

### For Software/Code References

```bibtex
@software{babel_engine_2025,
  title={Babel Engine: Thermodynamic Language Model Implementation},
  author={Adu, O.},
  year={2025},
  version={0.1},
  url={https://github.com/yourname/tlm-babel}
}
```

## Paper Title

**Thermodynamic Language Models: Emergent Structure in Discrete Energy-Based Text Systems**

## Abstract (Draft)

> We introduce Thermodynamic Language Models (TLMs), a new class of language models that represent text as discrete energy landscapes and generate structure via thermodynamic sampling instead of next-token prediction. Unlike autoregressive Transformers, TLMs assign energy to entire sequences and use Gibbs sampling to move from random noise toward structured, low-energy configurations. We present Babel Engine v0.1, a character-level TLM trained on random "Library of Babel"–style text that demonstrates how order can emerge from chaos. Despite being trained on pure noise, the model learns to form repeating patterns, stabilize bigrams, and converge to attractor states—showing that structure can emerge even in symbolic chaos. This work establishes TLMs as a physics-inspired alternative to traditional language models, with natural compatibility with probabilistic computing hardware.

## Keywords

- Thermodynamic Language Models
- Energy-Based Models
- Gibbs Sampling
- Symbolic Sequences
- Emergent Structure
- Probabilistic Computing

## Related Work

- LeCun, Y., et al. "A Tutorial on Energy-Based Learning" (2006)
- Extropic THRML Framework
- Borges, J. L. "The Library of Babel" (1941)
- Energy-Based Models for Text Generation

